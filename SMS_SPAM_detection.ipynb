{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31afc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import layers, Input, Model, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.random import set_seed\n",
    "#pip install keras-tuner --upgrade\n",
    "import keras_tuner as kt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f701e6d",
   "metadata": {},
   "source": [
    "1. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d53bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7327c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b37420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9a08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc97a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: v1, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v1'].value_counts()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275b440b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f22e6",
   "metadata": {},
   "source": [
    "How we can see our data set is unbalanced, 86.5% to 13.5% percent, in favor of not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1761e3c",
   "metadata": {},
   "source": [
    "2. Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66dbb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let transform our labels to binary values\n",
    "Y = LabelEncoder().fit_transform(data['v1']).reshape(-1,1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe6da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['v2']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb26436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8916"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check how many words we have in our dataset\n",
    "\n",
    "# tokenize the document\n",
    "words = []\n",
    "\n",
    "for i in range(0, X.shape[0]):\n",
    "    result = text_to_word_sequence(X[i])\n",
    "    words.extend(result)\n",
    "    \n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf826ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i      2351\n",
       "to     2242\n",
       "you    2150\n",
       "a      1433\n",
       "the    1328\n",
       "u      1172\n",
       "and     979\n",
       "in      898\n",
       "is      889\n",
       "me      802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let see top 10 words, whether there are some useless words or single characters\n",
    "pd.DataFrame(words).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d68e32",
   "metadata": {},
   "source": [
    "How we can see, many of our popular words are stopwords, let remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ec5384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Our stopwords which we'll remove are presented below\n",
    "#nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d000d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     v2  \\\n",
      "0     Go until jurong point, crazy.. Available only ...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3     U dun say so early hor... U c already then say...   \n",
      "4     Nah I don't think he goes to usf, he lives aro...   \n",
      "...                                                 ...   \n",
      "5567  This is the 2nd time we have tried 2 contact u...   \n",
      "5568              Will �_ b going to esplanade fr home?   \n",
      "5569  Pity, * was in mood for that. So...any other s...   \n",
      "5570  The guy did some bitching but I acted like i'd...   \n",
      "5571                         Rofl. Its true to its name   \n",
      "\n",
      "                                                  clean  \n",
      "0     Go jurong point, crazy.. Available bugis n gre...  \n",
      "1                         Ok lar... Joking wif u oni...  \n",
      "2     Free entry 2 wkly comp win FA Cup final tkts 2...  \n",
      "3             U dun say early hor... U c already say...  \n",
      "4               Nah think goes usf, lives around though  \n",
      "...                                                 ...  \n",
      "5567  2nd time tried 2 contact u. U �750 Pound prize...  \n",
      "5568                      �_ b going esplanade fr home?  \n",
      "5569           Pity, * mood that. So...any suggestions?  \n",
      "5570  guy bitching acted like i'd interested buying ...  \n",
      "5571                                    Rofl. true name  \n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df = pd.DataFrame(X)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "df['clean'] = df['v2'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (stop_words)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd11c03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u       1172\n",
       "call     591\n",
       "2        515\n",
       "i'm      394\n",
       "get      391\n",
       "ur       385\n",
       "gt       318\n",
       "lt       316\n",
       "4        316\n",
       "ok       287\n",
       "free     284\n",
       "go       280\n",
       "know     261\n",
       "now      255\n",
       "good     245\n",
       "like     245\n",
       "got      239\n",
       "you      233\n",
       "come     229\n",
       "time     220\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check again if there are still any useless words not included in stopwords\n",
    "\n",
    "# tokenize the document\n",
    "words = []\n",
    "df = df['clean']\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    result = text_to_word_sequence(df[i])\n",
    "    words.extend(result)\n",
    "    \n",
    "len(set(words))\n",
    "\n",
    "    \n",
    "pd.DataFrame(words).value_counts().head(20)\n",
    "#there are still some stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e90c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  clean  \\\n",
      "0     Go jurong point, crazy.. Available bugis n gre...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry 2 wkly comp win FA Cup final tkts 2...   \n",
      "3             U dun say early hor... U c already say...   \n",
      "4               Nah think goes usf, lives around though   \n",
      "...                                                 ...   \n",
      "5567  2nd time tried 2 contact u. U �750 Pound prize...   \n",
      "5568                      �_ b going esplanade fr home?   \n",
      "5569           Pity, * mood that. So...any suggestions?   \n",
      "5570  guy bitching acted like i'd interested buying ...   \n",
      "5571                                    Rofl. true name   \n",
      "\n",
      "                                                clean_1  \n",
      "0     Go jurong point, crazy.. Available bugis great...  \n",
      "1                           Ok lar... Joking wif oni...  \n",
      "2     Free entry wkly comp win FA Cup final tkts 21s...  \n",
      "3                 dun say early hor... c already say...  \n",
      "4               Nah think goes usf, lives around though  \n",
      "...                                                 ...  \n",
      "5567  2nd time tried contact u. �750 Pound prize. cl...  \n",
      "5568                      �_ b going esplanade fr home?  \n",
      "5569           Pity, * mood that. So...any suggestions?  \n",
      "5570  guy bitching acted like i'd interested buying ...  \n",
      "5571                                    Rofl. true name  \n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# let remove another stopwords, not defined before\n",
    "\n",
    "more_stopwords = set(['u','4','2',\"i'm\",\"i'll\",'r','ur','n'])\n",
    "updated_step_words = stop_words | more_stopwords\n",
    "\n",
    "df1 = pd.DataFrame(df)\n",
    "\n",
    "df1['clean_1'] = df1['clean'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (updated_step_words)]))\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d2d184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_1</th>\n",
       "      <th>text_steemed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go jurong point, crazy.. Available bugis n gre...</td>\n",
       "      <td>Go jurong point, crazy.. Available bugis great...</td>\n",
       "      <td>go jurong point , crazi .. avail bugi great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar... Joking wif oni...</td>\n",
       "      <td>ok lar ... joke wif oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry 2 wkly comp win FA Cup final tkts 2...</td>\n",
       "      <td>Free entry wkly comp win FA Cup final tkts 21s...</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt 21st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say early hor... U c already say...</td>\n",
       "      <td>dun say early hor... c already say...</td>\n",
       "      <td>dun say earli hor ... c alreadi say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah think goes usf, lives around though</td>\n",
       "      <td>Nah think goes usf, lives around though</td>\n",
       "      <td>nah think goe usf , live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>2nd time tried 2 contact u. U �750 Pound prize...</td>\n",
       "      <td>2nd time tried contact u. �750 Pound prize. cl...</td>\n",
       "      <td>2nd time tri contact u . �750 pound prize . cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>�_ b going esplanade fr home?</td>\n",
       "      <td>�_ b going esplanade fr home?</td>\n",
       "      <td>�_ b go esplanad fr home ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Pity, * mood that. So...any suggestions?</td>\n",
       "      <td>Pity, * mood that. So...any suggestions?</td>\n",
       "      <td>piti , * mood that . so ... ani suggest ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>guy bitching acted like i'd interested buying ...</td>\n",
       "      <td>guy bitching acted like i'd interested buying ...</td>\n",
       "      <td>guy bitch act like i 'd interest buy someth el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Rofl. true name</td>\n",
       "      <td>Rofl. true name</td>\n",
       "      <td>rofl . true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  clean  \\\n",
       "0     Go jurong point, crazy.. Available bugis n gre...   \n",
       "1                         Ok lar... Joking wif u oni...   \n",
       "2     Free entry 2 wkly comp win FA Cup final tkts 2...   \n",
       "3             U dun say early hor... U c already say...   \n",
       "4               Nah think goes usf, lives around though   \n",
       "...                                                 ...   \n",
       "5567  2nd time tried 2 contact u. U �750 Pound prize...   \n",
       "5568                      �_ b going esplanade fr home?   \n",
       "5569           Pity, * mood that. So...any suggestions?   \n",
       "5570  guy bitching acted like i'd interested buying ...   \n",
       "5571                                    Rofl. true name   \n",
       "\n",
       "                                                clean_1  \\\n",
       "0     Go jurong point, crazy.. Available bugis great...   \n",
       "1                           Ok lar... Joking wif oni...   \n",
       "2     Free entry wkly comp win FA Cup final tkts 21s...   \n",
       "3                 dun say early hor... c already say...   \n",
       "4               Nah think goes usf, lives around though   \n",
       "...                                                 ...   \n",
       "5567  2nd time tried contact u. �750 Pound prize. cl...   \n",
       "5568                      �_ b going esplanade fr home?   \n",
       "5569           Pity, * mood that. So...any suggestions?   \n",
       "5570  guy bitching acted like i'd interested buying ...   \n",
       "5571                                    Rofl. true name   \n",
       "\n",
       "                                           text_steemed  \n",
       "0     go jurong point , crazi .. avail bugi great wo...  \n",
       "1                           ok lar ... joke wif oni ...  \n",
       "2     free entri wkli comp win fa cup final tkt 21st...  \n",
       "3               dun say earli hor ... c alreadi say ...  \n",
       "4                nah think goe usf , live around though  \n",
       "...                                                 ...  \n",
       "5567  2nd time tri contact u . �750 pound prize . cl...  \n",
       "5568                         �_ b go esplanad fr home ?  \n",
       "5569          piti , * mood that . so ... ani suggest ?  \n",
       "5570  guy bitch act like i 'd interest buy someth el...  \n",
       "5571                                   rofl . true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let steem words in our dataset\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def porter(text):\n",
    "    text = nltk.word_tokenize(text) # split sentence to words\n",
    "    text_set = []                   # empty list\n",
    "    for i in text:                  # for each word do...\n",
    "        text_set.append(ps.stem(i)) # steem and append to list\n",
    "\n",
    "    return \" \".join(text_set)       # join every word in list back to sentence\n",
    "\n",
    "df1['text_steemed'] =df1['clean_1'].apply(porter)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c0ed49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let split our dataset on train and test\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(df1['text_steemed'],Y, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f3717",
   "metadata": {},
   "source": [
    "3. Text tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list musi byc lista slow z kazdego wiersza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "079f5ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let check the longest sms (number of words) in our dataset\n",
    "num_words = df1['text_steemed'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "num_words.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba2d389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words represents 99% percentile is  41.29\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#How looks the range of words per messege\n",
    "_= plt.hist(num_words, bins = 100)\n",
    "print('number of words represents 99% percentile is ', round(np.quantile(num_words,0.99),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fac329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let create tokenizer to apply values for each word\n",
    "max_words = 800\n",
    "max_len = 42\n",
    "\n",
    "tok = Tokenizer(max_words,lower= True)\n",
    "tok.fit_on_texts(X_train) #token trainging on X_train\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train) #conversion from text to vectors\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len) #matrix with padding has been created\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d6d83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let check records where after tokenizing we have only zeros\n",
    "test_0 = pd.DataFrame({\"X\" : test_sequences_matrix.argmax(axis=1)})\n",
    "test_0_filtred = test_0[test_0[\"X\"]<1]\n",
    "\n",
    "test_to_remove = test_0_filtred.index\n",
    "\n",
    "train_0 = pd.DataFrame({\"X\" : sequences_matrix.argmax(axis=1)})\n",
    "train_0_filtred = train_0[train_0[\"X\"]<1]\n",
    "\n",
    "train_to_remove = train_0_filtred.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96ba472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let remove those zeros tokens from our dataset \n",
    "\n",
    "#X_train \n",
    "sequences_matrix = np.delete(sequences_matrix, train_to_remove,axis = 0 )\n",
    "\n",
    "#X_test\n",
    "test_sequences_matrix =  np.delete(test_sequences_matrix, test_to_remove,axis = 0 )\n",
    "\n",
    "#Y_train\n",
    "Y_train = np.delete(Y_train, train_to_remove, axis = 0)\n",
    "\n",
    "#Y_test\n",
    "Y_test = np.delete(Y_test, test_to_remove, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c5dc4",
   "metadata": {},
   "source": [
    "4. Models implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69e0148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make reproduced results.\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2616a9",
   "metadata": {},
   "source": [
    "a) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7af0b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       960\n",
      "           1       0.82      0.45      0.58       139\n",
      "\n",
      "    accuracy                           0.92      1099\n",
      "   macro avg       0.87      0.72      0.77      1099\n",
      "weighted avg       0.91      0.92      0.91      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model1_svm = make_pipeline(MinMaxScaler(),svm.SVC())\n",
    "model1_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_svm = model1_svm.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4cce5f",
   "metadata": {},
   "source": [
    "As we can see, model predict not-spam class very well, but reach only 58% f1_score for spam class.  \n",
    "\n",
    "We are going to improve this model a little bit, by changing default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aa229fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       960\n",
      "           1       0.72      0.42      0.53       139\n",
      "\n",
      "    accuracy                           0.91      1099\n",
      "   macro avg       0.82      0.70      0.74      1099\n",
      "weighted avg       0.90      0.91      0.90      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another simple model\n",
    "\n",
    "model2_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'poly'))\n",
    "model2_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_svm = model2_svm.predict(test_sequences_matrix)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7de2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       960\n",
      "           1       0.57      0.25      0.35       139\n",
      "\n",
      "    accuracy                           0.88      1099\n",
      "   macro avg       0.74      0.61      0.64      1099\n",
      "weighted avg       0.86      0.88      0.86      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another simple model\n",
    "\n",
    "model3_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'linear'))\n",
    "model3_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred3_svm = model3_svm.predict(test_sequences_matrix)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred3_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c820763",
   "metadata": {},
   "source": [
    "Three basic models with default parameters, only with different kernel don't reach sufficient results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49be9b1",
   "metadata": {},
   "source": [
    "b) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0b7dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       960\n",
      "           1       0.67      0.47      0.56       139\n",
      "\n",
      "    accuracy                           0.90      1099\n",
      "   macro avg       0.80      0.72      0.75      1099\n",
      "weighted avg       0.89      0.90      0.90      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple model as benchmark\n",
    "\n",
    "model1_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n",
    "model1_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_knn = model1_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59def348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       960\n",
      "           1       0.78      0.63      0.70       139\n",
      "\n",
      "    accuracy                           0.93      1099\n",
      "   macro avg       0.86      0.80      0.83      1099\n",
      "weighted avg       0.93      0.93      0.93      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple model as benchmark\n",
    "\n",
    "model2_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier(weights = 'distance'))\n",
    "model2_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_knn = model2_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3bc9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       960\n",
      "           1       0.74      0.63      0.68       139\n",
      "\n",
      "    accuracy                           0.93      1099\n",
      "   macro avg       0.84      0.80      0.82      1099\n",
      "weighted avg       0.92      0.93      0.92      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple model as benchmark\n",
    "\n",
    "model3_knn = make_pipeline(StandardScaler(),KNeighborsClassifier(weights = 'distance'))\n",
    "model3_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred3_knn = model3_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred3_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d6856",
   "metadata": {},
   "source": [
    "How we can see first, basic KNN model didn't improve our previous results, but with only one change in weight parameters, we beat out previous best result. \n",
    "Finally with StandardScaler instead of MinMaxScaller our results was a little bit better.  \n",
    "Let try with more another option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3ddae",
   "metadata": {},
   "source": [
    "c) RandomForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "869c548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       960\n",
      "           1       0.87      0.65      0.75       139\n",
      "\n",
      "    accuracy                           0.94      1099\n",
      "   macro avg       0.91      0.82      0.86      1099\n",
      "weighted avg       0.94      0.94      0.94      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "model1_rf = RandomForestClassifier(random_state=1)\n",
    "model1_rf.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_rf = model1_rf.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea6fac",
   "metadata": {},
   "source": [
    "After first try, in default model we reached the highest f1 score on this dataset.\n",
    "Let check more sofisticated model form keras library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81339754",
   "metadata": {},
   "source": [
    "d) simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "736a5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to prepare Y_train and Y_test to correct shape\n",
    "Y_train_encoded = OneHotEncoder(sparse= False).fit_transform(Y_train)\n",
    "Y_test_encoded = OneHotEncoder(sparse= False).fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3d3c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set result repeatable\n",
    "\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92dd1bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 2s 5ms/step - loss: 2.3399 - auc: 0.8443 - val_loss: 2.1315 - val_auc: 0.8604\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 2.1208 - auc: 0.8608 - val_loss: 2.1314 - val_auc: 0.8604\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 2.1207 - auc: 0.8614 - val_loss: 2.1313 - val_auc: 0.8607\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 2.1207 - auc: 0.8615 - val_loss: 2.1313 - val_auc: 0.8607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       960\n",
      "           1       0.00      0.00      0.00       139\n",
      "\n",
      "    accuracy                           0.87      1099\n",
      "   macro avg       0.44      0.50      0.47      1099\n",
      "weighted avg       0.76      0.87      0.81      1099\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dawid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dawid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dawid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# first DL model\n",
    "input_layer = Input(shape = sequences_matrix.shape[1])\n",
    "\n",
    "x1 = layers.Dense(128, activation='relu')(input_layer)\n",
    "x1 = layers.Dense(64, activation='relu')(x1)\n",
    "\n",
    "out = layers.Dense(2)(x1)\n",
    "\n",
    "out = layers.Softmax()(out)\n",
    "\n",
    "model_dl1 = Model(inputs = input_layer, outputs = out)\n",
    "model_dl1.compile(optimizer= 'Adam', loss = 'binary_crossentropy', metrics = 'AUC')\n",
    "\n",
    "model_dl1.fit(sequences_matrix, Y_train_encoded, epochs = 20, validation_split=0.1, callbacks=EarlyStopping(patience=3, monitor='val_loss', min_delta = 0.02))\n",
    "\n",
    "y_pred_dl1 = model_dl1.predict(test_sequences_matrix).argmax(axis = 1) \n",
    "\n",
    "print(classification_report(Y_test.flatten(),y_pred_dl1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28264fce",
   "metadata": {},
   "source": [
    "How we can see, simple DL model hasn't improve our result.  \n",
    "Model with only dense layer can't predict spam class,\n",
    "let improve it, by adding recurential layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35ad0",
   "metadata": {},
   "source": [
    "e) rnn networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8545755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model definition\n",
    "def model_rnn(X_train,Y_train, units_rnn, embedd = 64, opt = 'adam', metrics = 'AUC', drop = 0, batch = 64, epochs = 30, verbose = True, display = 2):\n",
    "    \n",
    "    '''\n",
    "    X_train = training data without labels, in matrix format, prepared earlier as a sequences_matrix,\n",
    "    Y_train = labels for training data as an array(None, 2),\n",
    "    units_rnn = list of units per layer, same units per layer are recommended\n",
    "    embedd = dimension of embedding, default 64\n",
    "    opt = optimizer as a string,  default = 'adam'\n",
    "    drop = dropout size, default = 0\n",
    "    metric = metric to evaluate model results, default = 'auc'\n",
    "    batch = batch size, default 64, NOTE than batch size should be divided by 8\n",
    "    epochs = number of epochs, iteration of model, default = 30\n",
    "    verbose = whether print model summary or not, default = true (print)\n",
    "    display = whether print during fit progres bar (1), oneline summary per epoch (2) or nothing (0), default 2\n",
    "    \n",
    "    max_words = number of words used in model, parameter defined in preprocessing stage,\n",
    "    max_len = maximal lenght of one record, number of words in one record, parameter defined in preprocessing stage,\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # model instance\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    model.add(\n",
    "        layers.InputLayer(\n",
    "                    name = 'intro', \n",
    "                    input_shape = max_len\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # text embedding\n",
    "    model.add(\n",
    "        layers.Embedding(\n",
    "            input_dim = max_words,\n",
    "            output_dim=embedd\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model with LSTM layers, and dense as an output\n",
    "    for i, j in enumerate(units_rnn):\n",
    "        if i == len(units_rnn)-1 and i == 0:\n",
    "            model.add(\n",
    "                layers.LSTM(\n",
    "                    units = j, \n",
    "                    name = 'rnn' + str(i)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        elif i == len(units_rnn)-1:\n",
    "             model.add(\n",
    "                layers.LSTM(\n",
    "                    units = j, \n",
    "                    name = 'rnn' + str(i)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            model.add(\n",
    "                layers.LSTM(\n",
    "                    units = j, \n",
    "                    return_sequences = True,\n",
    "                    name = 'rnn' + str(i)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    model.add(layers.Dropout(rate = drop))\n",
    "            \n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=2,\n",
    "            name = 'out'\n",
    "            )\n",
    "        )  \n",
    "    \n",
    "    \n",
    "    model.add(layers.Softmax())\n",
    "        \n",
    "    if verbose == True:\n",
    "        model.summary()\n",
    "    \n",
    "    \n",
    "    # model compile\n",
    "    model.compile(\n",
    "            loss = 'binary_crossentropy',\n",
    "            optimizer = opt,\n",
    "            metrics = metrics\n",
    "    )\n",
    "    \n",
    "    # model fit\n",
    "    model.fit(\n",
    "        x = X_train,\n",
    "        y = Y_train,\n",
    "        batch_size = batch,\n",
    "        epochs = epochs,\n",
    "        validation_split = 0.2,\n",
    "        verbose = display,\n",
    "        callbacks = [EarlyStopping(\n",
    "                            monitor ='val_loss',\n",
    "                            patience = 5,\n",
    "                            min_delta = 0.01\n",
    "                            )]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "e8d476d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 70, 64)            51200     \n",
      "_________________________________________________________________\n",
      "rnn0 (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 84,354\n",
      "Trainable params: 84,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 36s 324ms/step - loss: 0.3352 - auc: 0.9381 - val_loss: 0.1541 - val_auc: 0.9860\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 28s 508ms/step - loss: 0.0683 - auc: 0.9963 - val_loss: 0.0840 - val_auc: 0.9957ss: \n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 29s 540ms/step - loss: 0.0347 - auc: 0.9991 - val_loss: 0.0779 - val_auc: 0.99638s - loss: 0.0457 - au - ETA: 6s - - ETA: 1s - loss: 0.0358 - auc:\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 15s 271ms/step - loss: 0.0190 - auc: 0.9996 - val_loss: 0.0863 - val_auc: 0.9922s - loss: 0.0201 - auc: \n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 22s 412ms/step - loss: 0.0111 - auc: 0.9999 - val_loss: 0.0869 - val_auc: 0.9907\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 34s 628ms/step - loss: 0.0095 - auc: 0.9999 - val_loss: 0.0965 - val_auc: 0.9900s: 0.010\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 16s 295ms/step - loss: 0.0062 - auc: 1.0000 - val_loss: 0.0942 - val_auc: 0.990065 - auc: 1. - ETA: 2s - loss: 0.0069 - auc:  - ETA: 1s - loss: 0.0065 - auc: 1.000 - ETA: 1s - loss: 0.0064  - ETA: 0s - loss: 0.0066 - auc: 1.000 - ETA: 0s - loss: 0.0065 - auc: 1.\n"
     ]
    }
   ],
   "source": [
    "model_rnn1 = model_rnn(sequences_matrix,Y_train_encoded,[64], display = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "7e9a7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.05\n",
      "  AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "# model evaluation results\n",
    "res = model_rnn1.evaluate(test_sequences_matrix,Y_test_encoded, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.2f}\\n  AUC: {:0.2f}'.format(res[0],res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "0e9fdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       948\n",
      "           1       0.98      0.94      0.96       139\n",
      "\n",
      "    accuracy                           0.99      1087\n",
      "   macro avg       0.98      0.97      0.97      1087\n",
      "weighted avg       0.99      0.99      0.99      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "y_pred = model_rnn1.predict(test_sequences_matrix).argmax(axis = 1)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9744e",
   "metadata": {},
   "source": [
    "How we can see our model, have almost perfect match, it makes only some mistakes, and not spam messeges classify as a spam.\n",
    "Let's go to optimize recall_score for 'spam' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "c4d57a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774436090225563"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision score for class 'spam'\n",
    "precision_score(Y_test, y_pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "791b8a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 - 12s - loss: 0.3261 - auc: 0.9371 - val_loss: 0.0965 - val_auc: 0.9934\n",
      "Epoch 2/30\n",
      "55/55 - 5s - loss: 0.0771 - auc: 0.9944 - val_loss: 0.0433 - val_auc: 0.9993\n",
      "Epoch 3/30\n",
      "55/55 - 5s - loss: 0.0409 - auc: 0.9987 - val_loss: 0.0352 - val_auc: 0.9994\n",
      "Epoch 4/30\n",
      "55/55 - 6s - loss: 0.0257 - auc: 0.9995 - val_loss: 0.0398 - val_auc: 0.9990\n",
      "Epoch 5/30\n",
      "55/55 - 6s - loss: 0.0169 - auc: 0.9995 - val_loss: 0.0397 - val_auc: 0.9993\n",
      "Epoch 6/30\n",
      "55/55 - 5s - loss: 0.0105 - auc: 0.9996 - val_loss: 0.0468 - val_auc: 0.9976\n",
      "Epoch 7/30\n",
      "55/55 - 6s - loss: 0.0102 - auc: 0.9996 - val_loss: 0.0540 - val_auc: 0.9982\n",
      "model1 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 - 9s - loss: 0.3258 - auc: 0.9402 - val_loss: 0.0951 - val_auc: 0.9973\n",
      "Epoch 2/30\n",
      "55/55 - 5s - loss: 0.0700 - auc: 0.9954 - val_loss: 0.0366 - val_auc: 0.9996\n",
      "Epoch 3/30\n",
      "55/55 - 5s - loss: 0.0388 - auc: 0.9990 - val_loss: 0.0279 - val_auc: 0.9996\n",
      "Epoch 4/30\n",
      "55/55 - 5s - loss: 0.0260 - auc: 0.9993 - val_loss: 0.0274 - val_auc: 0.9997\n",
      "Epoch 5/30\n",
      "55/55 - 5s - loss: 0.0171 - auc: 0.9997 - val_loss: 0.0250 - val_auc: 0.9997\n",
      "Epoch 6/30\n",
      "55/55 - 5s - loss: 0.0118 - auc: 0.9999 - val_loss: 0.0350 - val_auc: 0.9982\n",
      "Epoch 7/30\n",
      "55/55 - 5s - loss: 0.0089 - auc: 0.9999 - val_loss: 0.0316 - val_auc: 0.9984\n",
      "Epoch 8/30\n",
      "55/55 - 5s - loss: 0.0055 - auc: 1.0000 - val_loss: 0.0368 - val_auc: 0.9983\n",
      "Epoch 9/30\n",
      "55/55 - 5s - loss: 0.0046 - auc: 1.0000 - val_loss: 0.0407 - val_auc: 0.9972\n",
      "Epoch 10/30\n",
      "55/55 - 4s - loss: 0.0481 - auc: 0.9987 - val_loss: 0.0893 - val_auc: 0.9953\n",
      "model2 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 - 17s - loss: 0.2511 - auc: 0.9631 - val_loss: 0.0587 - val_auc: 0.9977\n",
      "Epoch 2/30\n",
      "55/55 - 11s - loss: 0.0611 - auc: 0.9964 - val_loss: 0.0403 - val_auc: 0.9993\n",
      "Epoch 3/30\n",
      "55/55 - 11s - loss: 0.0326 - auc: 0.9990 - val_loss: 0.0407 - val_auc: 0.9993\n",
      "Epoch 4/30\n",
      "55/55 - 13s - loss: 0.0194 - auc: 0.9994 - val_loss: 0.0488 - val_auc: 0.9956\n",
      "Epoch 5/30\n",
      "55/55 - 11s - loss: 0.0134 - auc: 0.9998 - val_loss: 0.0581 - val_auc: 0.9957\n",
      "Epoch 6/30\n",
      "55/55 - 11s - loss: 0.0104 - auc: 0.9996 - val_loss: 0.0584 - val_auc: 0.9968\n",
      "Epoch 7/30\n",
      "55/55 - 11s - loss: 0.0148 - auc: 0.9996 - val_loss: 0.0461 - val_auc: 0.9972\n",
      "model3 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 - 7s - loss: 0.3991 - auc: 0.9125 - val_loss: 0.1671 - val_auc: 0.9885\n",
      "Epoch 2/30\n",
      "55/55 - 2s - loss: 0.1194 - auc: 0.9913 - val_loss: 0.0556 - val_auc: 0.9991\n",
      "Epoch 3/30\n",
      "55/55 - 2s - loss: 0.0536 - auc: 0.9978 - val_loss: 0.0329 - val_auc: 0.9995\n",
      "Epoch 4/30\n",
      "55/55 - 2s - loss: 0.0332 - auc: 0.9993 - val_loss: 0.0371 - val_auc: 0.9994\n",
      "Epoch 5/30\n",
      "55/55 - 2s - loss: 0.0195 - auc: 0.9998 - val_loss: 0.0276 - val_auc: 0.9996\n",
      "Epoch 6/30\n",
      "55/55 - 2s - loss: 0.0126 - auc: 0.9999 - val_loss: 0.0367 - val_auc: 0.9992\n",
      "Epoch 7/30\n",
      "55/55 - 2s - loss: 0.0122 - auc: 0.9999 - val_loss: 0.0287 - val_auc: 0.9995\n",
      "Epoch 8/30\n",
      "55/55 - 2s - loss: 0.0065 - auc: 1.0000 - val_loss: 0.0301 - val_auc: 0.9995\n",
      "model4 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 - 10s - loss: 0.3583 - auc: 0.9254 - val_loss: 0.1289 - val_auc: 0.9917\n",
      "Epoch 2/30\n",
      "55/55 - 3s - loss: 0.0899 - auc: 0.9933 - val_loss: 0.0460 - val_auc: 0.9991\n",
      "Epoch 3/30\n",
      "55/55 - 3s - loss: 0.0434 - auc: 0.9985 - val_loss: 0.0343 - val_auc: 0.9994\n",
      "Epoch 4/30\n",
      "55/55 - 3s - loss: 0.0276 - auc: 0.9991 - val_loss: 0.0446 - val_auc: 0.9990\n",
      "Epoch 5/30\n",
      "55/55 - 3s - loss: 0.0192 - auc: 0.9997 - val_loss: 0.0410 - val_auc: 0.9993\n",
      "Epoch 6/30\n",
      "55/55 - 3s - loss: 0.0118 - auc: 0.9999 - val_loss: 0.0427 - val_auc: 0.9988\n",
      "Epoch 7/30\n",
      "55/55 - 3s - loss: 0.0107 - auc: 0.9999 - val_loss: 0.0450 - val_auc: 0.9991\n",
      "Epoch 8/30\n",
      "55/55 - 3s - loss: 0.0062 - auc: 1.0000 - val_loss: 0.0507 - val_auc: 0.9960\n",
      "model5 \n",
      "\n",
      "    units_rnn    recall\n",
      "0    [64, 64]  0.913669\n",
      "1       [128]  0.899281\n",
      "2  [128, 128]  0.913669\n",
      "3        [32]  0.935252\n",
      "4    [32, 32]  0.928058\n"
     ]
    }
   ],
   "source": [
    "parameters = { \"units_rnn\" : [[64,64],[128],[128,128],[32],[32,32]]}    \n",
    "\n",
    "results = pd.DataFrame({\"units_rnn\" : [], \"recall\" : []})\n",
    "\n",
    "lenght = len(parameters[\"units_rnn\"])\n",
    "\n",
    "for i in range(lenght):\n",
    "    param = parameters.get(\"units_rnn\")[i]\n",
    "    model = model_rnn(sequences_matrix,Y_train_encoded,param, verbose = False)\n",
    "    y_pred = model.predict(test_sequences_matrix).argmax(axis = 1)\n",
    "    print('model' + str(i+1), \"\\n\")\n",
    "    \n",
    "    recall = recall_score(Y_test, y_pred,pos_label=1)\n",
    "    results = results.append({\"units_rnn\": param,\"recall\" : recall}, ignore_index = True)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3bfad",
   "metadata": {},
   "source": [
    "How we can see the best results we achieved with less amount of units, the best with 32 units and one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "333987ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 42, 64)            51200     \n",
      "_________________________________________________________________\n",
      "rnn0 (LSTM)                  (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "softmax_7 (Softmax)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 63,682\n",
      "Trainable params: 63,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "55/55 - 7s - loss: 0.3797 - auc: 0.9242 - val_loss: 0.1543 - val_auc: 0.9887\n",
      "Epoch 2/30\n",
      "55/55 - 2s - loss: 0.1135 - auc: 0.9907 - val_loss: 0.0542 - val_auc: 0.9991\n",
      "Epoch 3/30\n",
      "55/55 - 2s - loss: 0.0548 - auc: 0.9974 - val_loss: 0.0338 - val_auc: 0.9994\n",
      "Epoch 4/30\n",
      "55/55 - 2s - loss: 0.0360 - auc: 0.9989 - val_loss: 0.0365 - val_auc: 0.9995\n",
      "Epoch 5/30\n",
      "55/55 - 2s - loss: 0.0220 - auc: 0.9996 - val_loss: 0.0271 - val_auc: 0.9996\n",
      "Epoch 6/30\n",
      "55/55 - 2s - loss: 0.0155 - auc: 0.9995 - val_loss: 0.0277 - val_auc: 0.9996\n",
      "Epoch 7/30\n",
      "55/55 - 2s - loss: 0.0122 - auc: 0.9999 - val_loss: 0.0282 - val_auc: 0.9996\n",
      "Epoch 8/30\n",
      "55/55 - 2s - loss: 0.0076 - auc: 1.0000 - val_loss: 0.0299 - val_auc: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9774436090225563"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let try with some droupout\n",
    "model_rnn2 = model_rnn(sequences_matrix,Y_train_encoded,[32], drop = 0.2)\n",
    "y_pred2 = model_rnn2.predict(test_sequences_matrix).argmax(axis = 1)\n",
    "precision_score(Y_test, y_pred2,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "baeab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       960\n",
      "           1       0.98      0.94      0.96       139\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a03200",
   "metadata": {},
   "source": [
    "How we can see there is no impact on results with dropout layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ce099",
   "metadata": {},
   "source": [
    "f) Keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65fc6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let try improve our model with keras tuner\n",
    "# first create model instance\n",
    "\n",
    "def build_model(hp):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        model.add(\n",
    "        layers.InputLayer(input_shape = max_len)\n",
    "        )\n",
    "        \n",
    "        # text embedding\n",
    "        model.add(\n",
    "            layers.Embedding(\n",
    "                input_dim = max_words,\n",
    "                output_dim=hp.Int(\n",
    "                'embedding_size', min_value = 15, max_value = 100, step = 5, default = 50)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(\n",
    "            layers.LSTM(\n",
    "                hp.Choice('units', [8,16,32,64,128]),\n",
    "                        name = 'rnn1'\n",
    "            )\n",
    "        )\n",
    "          \n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=2,\n",
    "                name = 'out'\n",
    "            )\n",
    "        )     \n",
    "        \n",
    "        model.add(layers.Softmax())  \n",
    "\n",
    "        model.compile(\n",
    "                loss = 'binary_crossentropy',\n",
    "                optimizer = optimizers.Adam(\n",
    "                    hp.Float(\n",
    "                        'learning_rate',\n",
    "                        min_value=1e-4,\n",
    "                        max_value=1e-2,\n",
    "                        sampling='LOG',\n",
    "                        default=1e-3\n",
    "                    )),\n",
    "                metrics = 'AUC'\n",
    "        )\n",
    "              \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9aeb46f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 08s]\n",
      "val_loss: 0.03451862931251526\n",
      "\n",
      "Best val_loss So Far: 0.027900947257876396\n",
      "Total elapsed time: 00h 06m 13s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# let use Bayesian Optimization definition and start to search the best parameters set\n",
    "tuner = kt.BayesianOptimization(build_model, objective = 'val_loss',max_trials = 30,overwrite = True)\n",
    "\n",
    "tuner.search(sequences_matrix,Y_train_encoded,\n",
    "        validation_split = 0.2,\n",
    "        callbacks = [EarlyStopping(\n",
    "                            monitor ='val_loss',\n",
    "                            patience = 5,\n",
    "                            min_delta = 0.01\n",
    "                            )]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19e8e4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9640287769784173"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model create,\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "y_pred_best = best_model.predict(test_sequences_matrix).argmax(axis = 1)\n",
    "recall_score(Y_test, y_pred_best,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d56d45dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 42, 40)            32000     \n",
      "_________________________________________________________________\n",
      "rnn1 (LSTM)                  (None, 8)                 1568      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 33,586\n",
      "Trainable params: 33,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{'embedding_size': 40, 'units': 8, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "#Our best model structure\n",
    "best_model.summary()\n",
    "\n",
    "#and set of parameters\n",
    "print(tuner.get_best_hyperparameters(1)[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3735987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       960\n",
      "           1       0.99      0.96      0.98       139\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.99      0.98      0.99      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8f917",
   "metadata": {},
   "source": [
    "5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ce6f3",
   "metadata": {},
   "source": [
    "In this case we had to detect spam in text messages.  \n",
    "\n",
    "The best results we achived using neural network with:\n",
    "- 1 recurential layer and 8 Units,\n",
    "- emedding layer with size 40,\n",
    "- Dense layerwith 2 units and softmax activation layers as an output \n",
    "- binary_crossentropy loss function,\n",
    "- Adam Optimizer with learning rate = 0.01\n",
    "\n",
    "\n",
    "Our model results is:\n",
    "- 99% accuracy in general, \n",
    "- 99% precision for both classes,\n",
    "- 96% recall in Spam class.\n",
    "\n",
    "Our results tell us that final model can detect 99% of spam, classifing correctly certain message as a spam with 96% probability (recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec65c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
