{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31afc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import layers, Input, Model, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f701e6d",
   "metadata": {},
   "source": [
    "1. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d53bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7327c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b37420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9a08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdc97a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: v1, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v1'].value_counts()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275b440b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f22e6",
   "metadata": {},
   "source": [
    "How we can see our data set is unbalanced, 86.5% to 13.5% percent, in favor of not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1761e3c",
   "metadata": {},
   "source": [
    "2. Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66dbb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let transform our labels to binary values\n",
    "Y = LabelEncoder().fit_transform(data['v1']).reshape(-1,1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe6da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['v2']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb26436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8916"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check how many words we have in our dataset\n",
    "\n",
    "# tokenize the document\n",
    "words = []\n",
    "\n",
    "for i in range(0, X.shape[0]):\n",
    "    result = text_to_word_sequence(X[i])\n",
    "    words.extend(result)\n",
    "    \n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf826ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i      2351\n",
       "to     2242\n",
       "you    2150\n",
       "a      1433\n",
       "the    1328\n",
       "u      1172\n",
       "and     979\n",
       "in      898\n",
       "is      889\n",
       "me      802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let see top 10 words, whether there are some useless words or single characters\n",
    "pd.DataFrame(words).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d68e32",
   "metadata": {},
   "source": [
    "How we can see, many of our popular words are stopwords, let remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ec5384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Our stopwords which we'll remove are presented below\n",
    "#nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d000d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     v2  \\\n",
      "0     Go until jurong point, crazy.. Available only ...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3     U dun say so early hor... U c already then say...   \n",
      "4     Nah I don't think he goes to usf, he lives aro...   \n",
      "...                                                 ...   \n",
      "5567  This is the 2nd time we have tried 2 contact u...   \n",
      "5568              Will �_ b going to esplanade fr home?   \n",
      "5569  Pity, * was in mood for that. So...any other s...   \n",
      "5570  The guy did some bitching but I acted like i'd...   \n",
      "5571                         Rofl. Its true to its name   \n",
      "\n",
      "                                                  clean  \n",
      "0     Go jurong point, crazy.. Available bugis n gre...  \n",
      "1                         Ok lar... Joking wif u oni...  \n",
      "2     Free entry 2 wkly comp win FA Cup final tkts 2...  \n",
      "3             U dun say early hor... U c already say...  \n",
      "4               Nah think goes usf, lives around though  \n",
      "...                                                 ...  \n",
      "5567  2nd time tried 2 contact u. U �750 Pound prize...  \n",
      "5568                      �_ b going esplanade fr home?  \n",
      "5569           Pity, * mood that. So...any suggestions?  \n",
      "5570  guy bitching acted like i'd interested buying ...  \n",
      "5571                                    Rofl. true name  \n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df = pd.DataFrame(X)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "df['clean'] = df['v2'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (stop_words)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c73e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let remove stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # tokenize our text\n",
    "# X_clean = []\n",
    "\n",
    "# for i in range(X.shape[0]):\n",
    "#     #tokenize each sentences\n",
    "#     word_tokens = text_to_word_sequence(X[i])\n",
    "#     filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] # filter stop words from orignal sentences\n",
    "#     X_clean.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd11c03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u       1172\n",
       "call     591\n",
       "2        515\n",
       "i'm      394\n",
       "get      391\n",
       "ur       385\n",
       "gt       318\n",
       "lt       316\n",
       "4        316\n",
       "ok       287\n",
       "free     284\n",
       "go       280\n",
       "know     261\n",
       "now      255\n",
       "good     245\n",
       "like     245\n",
       "got      239\n",
       "you      233\n",
       "come     229\n",
       "time     220\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check again if there are still any useless words not included in stopwords\n",
    "\n",
    "# tokenize the document\n",
    "words = []\n",
    "df = df['clean']\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    result = text_to_word_sequence(df[i])\n",
    "    words.extend(result)\n",
    "    \n",
    "len(set(words))\n",
    "\n",
    "    \n",
    "pd.DataFrame(words).value_counts().head(20)\n",
    "#there are still some stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e90c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  clean  \\\n",
      "0     Go jurong point, crazy.. Available bugis n gre...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry 2 wkly comp win FA Cup final tkts 2...   \n",
      "3             U dun say early hor... U c already say...   \n",
      "4               Nah think goes usf, lives around though   \n",
      "...                                                 ...   \n",
      "5567  2nd time tried 2 contact u. U �750 Pound prize...   \n",
      "5568                      �_ b going esplanade fr home?   \n",
      "5569           Pity, * mood that. So...any suggestions?   \n",
      "5570  guy bitching acted like i'd interested buying ...   \n",
      "5571                                    Rofl. true name   \n",
      "\n",
      "                                                clean_1  \n",
      "0     Go jurong point, crazy.. Available bugis great...  \n",
      "1                           Ok lar... Joking wif oni...  \n",
      "2     Free entry wkly comp win FA Cup final tkts 21s...  \n",
      "3                 dun say early hor... c already say...  \n",
      "4               Nah think goes usf, lives around though  \n",
      "...                                                 ...  \n",
      "5567  2nd time tried contact u. �750 Pound prize. cl...  \n",
      "5568                      �_ b going esplanade fr home?  \n",
      "5569           Pity, * mood that. So...any suggestions?  \n",
      "5570  guy bitching acted like i'd interested buying ...  \n",
      "5571                                    Rofl. true name  \n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# let remove another stopwords, not defined before\n",
    "\n",
    "more_stopwords = set(['u','4','2',\"i'm\",\"i'll\",'r','ur','n'])\n",
    "updated_step_words = stop_words | more_stopwords\n",
    "\n",
    "df1 = pd.DataFrame(df)\n",
    "\n",
    "df1['clean_1'] = df1['clean'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (updated_step_words)]))\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d2d184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go jurong point, crazy.. Available bugis great...\n",
       "1                             Ok lar... Joking wif oni...\n",
       "2       Free entry wkly comp win FA Cup final tkts 21s...\n",
       "3                   dun say early hor... c already say...\n",
       "4                 Nah think goes usf, lives around though\n",
       "                              ...                        \n",
       "5567    2nd time tried contact u. �750 Pound prize. cl...\n",
       "5568                        �_ b going esplanade fr home?\n",
       "5569             Pity, * mood that. So...any suggestions?\n",
       "5570    guy bitching acted like i'd interested buying ...\n",
       "5571                                      Rofl. true name\n",
       "Name: clean_1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['clean_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0ed49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let split our dataset on train and test\n",
    "\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(df1['clean_1'],Y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f3717",
   "metadata": {},
   "source": [
    "3. Text tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list musi byc lista slow z kazdego wiersza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "079f5ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let check the longest sms in our dataset\n",
    "maxims = []\n",
    "for i,j in enumerate(X_list):\n",
    "    maxims.append(len(X_list[i]))\n",
    "    \n",
    "np.max(maxims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fac329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let create tokenizer\n",
    "max_words = 800\n",
    "max_len = 76\n",
    "\n",
    "tok = Tokenizer(max_words,lower= True)\n",
    "tok.fit_on_texts(X_train) #token trainging on X_train\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train) #conversion from text to vectors\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len) #matrix with padding has been created\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d6d83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let check records where after tokenizing we have only zeros\n",
    "test_0 = pd.DataFrame({\"X\" : test_sequences_matrix.argmax(axis=1)})\n",
    "test_0_filtred = test_0[test_0[\"X\"]<1]\n",
    "\n",
    "test_to_remove = test_0_filtred.index\n",
    "\n",
    "train_0 = pd.DataFrame({\"X\" : sequences_matrix.argmax(axis=1)})\n",
    "train_0_filtred = train_0[train_0[\"X\"]<1]\n",
    "\n",
    "train_to_remove = train_0_filtred.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ba472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let remove those zeros tokens from our dataset \n",
    "\n",
    "#X_train \n",
    "sequences_matrix = np.delete(sequences_matrix, train_to_remove,axis = 0 )\n",
    "\n",
    "#X_test\n",
    "test_sequences_matrix =  np.delete(test_sequences_matrix, test_to_remove,axis = 0 )\n",
    "\n",
    "#Y_train\n",
    "Y_train = np.delete(Y_train, train_to_remove, axis = 0)\n",
    "\n",
    "#Y_test\n",
    "Y_test = np.delete(Y_test, test_to_remove, axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c5dc4",
   "metadata": {},
   "source": [
    "4. First models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "69e0148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make reproduced results.\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forrest, knn, svm, rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2616a9",
   "metadata": {},
   "source": [
    "a) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "7af0b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       948\n",
      "           1       0.75      0.37      0.50       139\n",
      "\n",
      "    accuracy                           0.90      1087\n",
      "   macro avg       0.83      0.68      0.72      1087\n",
      "weighted avg       0.89      0.90      0.89      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model1_svm = make_pipeline(MinMaxScaler(),svm.SVC())\n",
    "model1_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_svm = model1_svm.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4cce5f",
   "metadata": {},
   "source": [
    "As we can see, model predict not-spam class very well, but reach only 50% f1_score for spam class. \n",
    "We are going to improve this model a little bit, by changing default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "8aa229fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       948\n",
      "           1       0.77      0.36      0.49       139\n",
      "\n",
      "    accuracy                           0.90      1087\n",
      "   macro avg       0.84      0.67      0.72      1087\n",
      "weighted avg       0.89      0.90      0.89      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another simple model\n",
    "\n",
    "model2_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'poly'))\n",
    "model2_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_svm = model2_svm.predict(test_sequences_matrix)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "d7de2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       948\n",
      "           1       0.62      0.32      0.42       139\n",
      "\n",
      "    accuracy                           0.89      1087\n",
      "   macro avg       0.76      0.65      0.68      1087\n",
      "weighted avg       0.87      0.89      0.87      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another simple model\n",
    "\n",
    "model3_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'linear'))\n",
    "model3_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred3_svm = model3_svm.predict(test_sequences_matrix)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred3_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c820763",
   "metadata": {},
   "source": [
    "Three basic models with default parameters, only with different kernel don't reach sufficient results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49be9b1",
   "metadata": {},
   "source": [
    "b) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "d0b7dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94       948\n",
      "           1       0.60      0.38      0.47       139\n",
      "\n",
      "    accuracy                           0.89      1087\n",
      "   macro avg       0.76      0.67      0.70      1087\n",
      "weighted avg       0.87      0.89      0.88      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model1_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n",
    "model1_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_knn = model1_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "59def348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       948\n",
      "           1       0.74      0.53      0.62       139\n",
      "\n",
      "    accuracy                           0.92      1087\n",
      "   macro avg       0.84      0.75      0.79      1087\n",
      "weighted avg       0.91      0.92      0.91      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model2_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier(weights = 'distance'))\n",
    "model2_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_knn = model2_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "c3bc9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       948\n",
      "           1       0.75      0.57      0.64       139\n",
      "\n",
      "    accuracy                           0.92      1087\n",
      "   macro avg       0.84      0.77      0.80      1087\n",
      "weighted avg       0.91      0.92      0.92      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model2_knn = make_pipeline(StandardScaler(),KNeighborsClassifier(weights = 'distance'))\n",
    "model2_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_knn = model2_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d6856",
   "metadata": {},
   "source": [
    "How we can see first, basic KNN model didn't improve our previous results, but with only one change in weight parameters, we beat out previous best result. \n",
    "Finally with StandardScaler instead of MinMaxScaller our results was a little bit better.  \n",
    "Let try with more another option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3ddae",
   "metadata": {},
   "source": [
    "c) RandomForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "869c548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       948\n",
      "           1       0.89      0.59      0.71       139\n",
      "\n",
      "    accuracy                           0.94      1087\n",
      "   macro avg       0.92      0.79      0.84      1087\n",
      "weighted avg       0.94      0.94      0.93      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model1_rf = RandomForestClassifier(random_state=1)\n",
    "model1_rf.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_rf = model1_rf.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea6fac",
   "metadata": {},
   "source": [
    "After first try, in default model we reached the highest f1 score on this dataset.\n",
    "Let check more sofisticated model form keras library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81339754",
   "metadata": {},
   "source": [
    "d) simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "736a5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to prepare Y_train and Y_test to correct shape\n",
    "Y_train_encoded = OneHotEncoder(sparse= False).fit_transform(Y_train)\n",
    "Y_test_encoded = OneHotEncoder(sparse= False).fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3d3c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set result repeatable\n",
    "\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "92dd1bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "123/123 [==============================] - 21s 117ms/step - loss: 2.0635 - auc: 0.8618 - val_loss: 2.0742 - val_auc: 0.8610\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 13s 103ms/step - loss: 2.0564 - auc: 0.8632 - val_loss: 2.0712 - val_auc: 0.8610\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 14s 114ms/step - loss: 2.0556 - auc: 0.8633 - val_loss: 2.0709 - val_auc: 0.8610\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 13s 104ms/step - loss: 2.0555 - auc: 0.8652 - val_loss: 2.0708 - val_auc: 0.8650\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 14s 111ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 11s 94ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 15s 124ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650- auc: - ETA: 0s - loss: 2.0639 - auc: 0\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 11s 93ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 13s 104ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650- loss: 1.9651 - au\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 12s 96ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 11s 89ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650 2.2312 - auc: \n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 12s 101ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 8s 65ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 10s 85ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 9s 75ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 10s 84ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650 - loss: 2.0495 - auc: 0.86\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 9s 76ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650ss: 1.9575 - auc: 0.8\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 8s 67ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 12s 99ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 9s 78ms/step - loss: 2.0554 - auc: 0.8660 - val_loss: 2.0707 - val_auc: 0.8650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       948\n",
      "           1       0.88      0.58      0.70       139\n",
      "\n",
      "    accuracy                           0.94      1087\n",
      "   macro avg       0.91      0.79      0.83      1087\n",
      "weighted avg       0.93      0.94      0.93      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first DL model\n",
    "input_layer = Input(shape = sequences_matrix.shape[1])\n",
    "\n",
    "x1 = layers.Dense(128, activation='relu')(input_layer)\n",
    "x1 = layers.Dense(64, activation='relu')(x1)\n",
    "\n",
    "out = layers.Dense(2)(x1)\n",
    "\n",
    "out = layers.Softmax()(out)\n",
    "\n",
    "model_dl1 = Model(inputs = input_layer, outputs = out)\n",
    "model_dl1.compile(optimizer= 'Adam', loss = 'binary_crossentropy', metrics = 'AUC')\n",
    "\n",
    "model_dl1.fit(sequences_matrix, Y_train_encoded, epochs = 20, validation_split=0.1, callbacks=EarlyStopping(patience=3, monitor='val_loss'))\n",
    "\n",
    "y_pred_dl1 = model_dl1.predict(test_sequences_matrix).argmax(axis = 1) \n",
    "\n",
    "print(classification_report(Y_test.flatten(),y_pred1_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28264fce",
   "metadata": {},
   "source": [
    "How we can see, simple DL model hasn't improve our result, let improve it, by adding recurential layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35ad0",
   "metadata": {},
   "source": [
    "e) rnn networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8545755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model definition\n",
    "def model_rnn(X_train,Y_train, units_rnn, embedd = 64, opt = 'adam', metrics = 'AUC', batch = 64, epochs = 30, verbose = True):\n",
    "    \n",
    "    '''\n",
    "    X_train = training data without labels, in matrix format, prepared earlier as a sequences_matrix,\n",
    "    Y_train = labels for training data as an array(None, 2),\n",
    "    units_rnn = list of units per layer, same units per layer are recommended\n",
    "    embedd = dimension of embedding, default 64\n",
    "    opt = optimizer as a string,  default = 'adam'\n",
    "    metric = metric to evaluate model results, default = 'auc'\n",
    "    batch = batch size, default 64, NOTE than batch size should be divided by 8\n",
    "    epochs = number of epochs, iteration of model, default = 30\n",
    "    verbose = whether print model summary or not, default = true (print)\n",
    "    \n",
    "    \n",
    "    max_words = number of words used in model, parameter defined in preprocessing stage,\n",
    "    max_len = maximal lenght of one record, number of words in one record, parameter defined in preprocessing stage,\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # model instance\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    model.add(\n",
    "        layers.InputLayer(\n",
    "                    name = 'intro', \n",
    "                    input_shape = max_len\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # text embedding\n",
    "    model.add(\n",
    "        layers.Embedding(\n",
    "            input_dim = max_words,\n",
    "            output_dim=embedd\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model with LSTM layers, and dense as an output\n",
    "    for i, j in enumerate(units_rnn):\n",
    "        if i == len(units_rnn)-1 and i == 0:\n",
    "            model.add(\n",
    "                layers.LSTM(\n",
    "                    units = j, \n",
    "                    name = 'rnn' + str(i)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        elif i == len(units_rnn)-1:\n",
    "             model.add(\n",
    "                layers.LSTM(\n",
    "                    units = j, \n",
    "                    name = 'rnn' + str(i)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            model.add(\n",
    "                layers.LSTM(\n",
    "                    units = j, \n",
    "                    return_sequences = True,\n",
    "                    name = 'rnn' + str(i)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=2,\n",
    "            name = 'out'\n",
    "            )\n",
    "        )  \n",
    "    \n",
    "    \n",
    "    model.add(layers.Softmax())\n",
    "    \n",
    "#     model.add(\n",
    "#         layers.Dense(\n",
    "#             units=1,\n",
    "#             activation=\"sigmoid\",\n",
    "#             name = 'out'\n",
    "#             )\n",
    "#         )            \n",
    "    \n",
    "    if verbose == True:\n",
    "        model.summary()\n",
    "    \n",
    "    \n",
    "    # model compile\n",
    "    model.compile(\n",
    "            loss = 'binary_crossentropy',\n",
    "            optimizer = opt,\n",
    "            metrics = metrics\n",
    "    )\n",
    "    \n",
    "    # model fit\n",
    "    model.fit(\n",
    "        x = X_train,\n",
    "        y = Y_train,\n",
    "        batch_size = batch,\n",
    "        epochs = epochs,\n",
    "        validation_split = 0.2,\n",
    "        callbacks = [EarlyStopping(\n",
    "                            monitor ='val_loss',\n",
    "                            patience = 5,\n",
    "                            min_delta = 0.01\n",
    "                            )]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model    \n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "e8d476d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 70, 64)            51200     \n",
      "_________________________________________________________________\n",
      "rnn0 (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 84,354\n",
      "Trainable params: 84,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 36s 324ms/step - loss: 0.3352 - auc: 0.9381 - val_loss: 0.1541 - val_auc: 0.9860\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 28s 508ms/step - loss: 0.0683 - auc: 0.9963 - val_loss: 0.0840 - val_auc: 0.9957ss: \n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 29s 540ms/step - loss: 0.0347 - auc: 0.9991 - val_loss: 0.0779 - val_auc: 0.99638s - loss: 0.0457 - au - ETA: 6s - - ETA: 1s - loss: 0.0358 - auc:\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 15s 271ms/step - loss: 0.0190 - auc: 0.9996 - val_loss: 0.0863 - val_auc: 0.9922s - loss: 0.0201 - auc: \n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 22s 412ms/step - loss: 0.0111 - auc: 0.9999 - val_loss: 0.0869 - val_auc: 0.9907\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 34s 628ms/step - loss: 0.0095 - auc: 0.9999 - val_loss: 0.0965 - val_auc: 0.9900s: 0.010\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 16s 295ms/step - loss: 0.0062 - auc: 1.0000 - val_loss: 0.0942 - val_auc: 0.990065 - auc: 1. - ETA: 2s - loss: 0.0069 - auc:  - ETA: 1s - loss: 0.0065 - auc: 1.000 - ETA: 1s - loss: 0.0064  - ETA: 0s - loss: 0.0066 - auc: 1.000 - ETA: 0s - loss: 0.0065 - auc: 1.\n"
     ]
    }
   ],
   "source": [
    "model_rnn1 = model_rnn(sequences_matrix,Y_train_encoded,[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "7e9a7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.05\n",
      "  AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "# model evaluation results\n",
    "res = model_rnn1.evaluate(test_sequences_matrix,Y_test_encoded, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.2f}\\n  AUC: {:0.2f}'.format(res[0],res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "0e9fdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       948\n",
      "           1       0.98      0.94      0.96       139\n",
      "\n",
      "    accuracy                           0.99      1087\n",
      "   macro avg       0.98      0.97      0.97      1087\n",
      "weighted avg       0.99      0.99      0.99      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "y_pred = model_rnn1.predict(test_sequences_matrix).argmax(axis = 1)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9744e",
   "metadata": {},
   "source": [
    "How we can see our model, have almost perfect match, it makes only some mistakes, and not spam messeges classify as a spam.\n",
    "Let's go to optimize recall_score for 'spam' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "c4d57a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774436090225563"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision score for class 'spam'\n",
    "precision_score(Y_test, y_pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "791b8a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 26s 264ms/step - loss: 0.3199 - auc: 0.9402 - val_loss: 0.1283 - val_auc: 0.9953: 9s - loss: 0.5353 - auc: 0.8 - ETA: 8s - loss: 0.5053 - auc: 0.875 - ETA: 8s - loss: 0.4945 - auc: - ETA: 6s - loss: 0.4475 - auc: 0.88 - ETA: 6s - loss: 0.4324 - auc: 0.894 - ETA: 6s - loss: 0.4283 - auc: 0.8 - ETA: 5s - loss: 0.4140 - auc: 0.900 - ETA: 5s - loss: 0.4080 - auc: 0.9 - ETA: 4s - loss: 0.3926 - auc: 0.910 - ETA: 4s - loss: 0.3920 - auc: 0.909 - ETA: 4s - loss: 0.3883 - auc: 0.9 - ETA: 3s - loss: 0.3782 - auc: 0.915 - ETA: 3s - loss: 0.3734 - auc: 0.918 - ETA: 2s - loss: 0.3699 - auc - ETA: 1s - loss: 0.3362 - auc: 0. - ETA: 0s - loss: 0.3242 - auc: 0.9\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 15s 275ms/step - loss: 0.0652 - auc: 0.9961 - val_loss: 0.0493 - val_auc: 0.9978oss: 0.1043 - auc - ETA: 9s - loss: 0.1017 - auc: 0. - ETA: 8s - loss: 0.0871 - auc: 0.9 - ETA: 7s - loss: 0.086 - ETA: 4s - loss: 0.0716 - auc: 0. - ETA: 3s - loss: 0.0733 - au - ETA: 1s - loss: 0.0699 - auc:\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 15s 277ms/step - loss: 0.0330 - auc: 0.9988 - val_loss: 0.0501 - val_auc: 0.9966s: 0.0263 - auc: 0.99 - ETA: 10s - loss: 0.0274 - au - ETA: 8s - loss: 0.0412 - auc: 0.99 - ETA: 7s - loss: 0.0420 - auc: 0.998 - ETA:\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 13s 226ms/step - loss: 0.0201 - auc: 0.9994 - val_loss: 0.0721 - val_auc: 0.9956\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 12s 219ms/step - loss: 0.0151 - auc: 0.9998 - val_loss: 0.0639 - val_auc: 0.9958\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 11s 198ms/step - loss: 0.0098 - auc: 0.9996 - val_loss: 0.0664 - val_auc: 0.9958\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 11s 193ms/step - loss: 0.0076 - auc: 0.9997 - val_loss: 0.0673 - val_auc: 0.99580.\n",
      "model1 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 15s 220ms/step - loss: 0.3069 - auc: 0.9463 - val_loss: 0.1040 - val_auc: 0.9959\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 10s 175ms/step - loss: 0.0643 - auc: 0.9960 - val_loss: 0.0497 - val_auc: 0.9977\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 10s 178ms/step - loss: 0.0339 - auc: 0.9990 - val_loss: 0.0413 - val_auc: 0.9979\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 10s 175ms/step - loss: 0.0202 - auc: 0.9997 - val_loss: 0.0417 - val_auc: 0.9978\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 10s 175ms/step - loss: 0.0143 - auc: 0.9996 - val_loss: 0.0552 - val_auc: 0.9971\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 10s 178ms/step - loss: 0.0175 - auc: 0.9996 - val_loss: 0.0619 - val_auc: 0.9971\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 9s 169ms/step - loss: 0.0189 - auc: 0.9996 - val_loss: 0.0440 - val_auc: 0.9980\n",
      "model2 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 33s 498ms/step - loss: 0.2728 - auc: 0.9555 - val_loss: 0.0816 - val_auc: 0.9972\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 25s 456ms/step - loss: 0.0519 - auc: 0.9970 - val_loss: 0.0470 - val_auc: 0.9969\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 26s 476ms/step - loss: 0.0290 - auc: 0.9988 - val_loss: 0.0482 - val_auc: 0.9958\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 25s 461ms/step - loss: 0.0165 - auc: 0.9995 - val_loss: 0.0583 - val_auc: 0.9965\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 26s 475ms/step - loss: 0.0122 - auc: 0.9996 - val_loss: 0.0618 - val_auc: 0.9959\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 25s 461ms/step - loss: 0.0082 - auc: 0.9997 - val_loss: 0.0644 - val_auc: 0.9960\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 26s 469ms/step - loss: 0.0068 - auc: 0.9997 - val_loss: 0.0611 - val_auc: 0.9959\n",
      "model3 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 7s 74ms/step - loss: 0.3575 - auc: 0.9327 - val_loss: 0.1399 - val_auc: 0.9921\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 0.0908 - auc: 0.9944 - val_loss: 0.0533 - val_auc: 0.9970\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 0.0442 - auc: 0.9982 - val_loss: 0.0385 - val_auc: 0.9989\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 0.0266 - auc: 0.9995 - val_loss: 0.0375 - val_auc: 0.9988\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 0.0161 - auc: 0.9996 - val_loss: 0.0383 - val_auc: 0.9980\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 0.0102 - auc: 0.9997 - val_loss: 0.0378 - val_auc: 0.9972\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 3s 59ms/step - loss: 0.0076 - auc: 0.9997 - val_loss: 0.0378 - val_auc: 0.9971\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 0.0058 - auc: 0.9997 - val_loss: 0.0404 - val_auc: 0.9972\n",
      "model4 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 12s 126ms/step - loss: 0.3609 - auc: 0.9255 - val_loss: 0.1681 - val_auc: 0.9922- loss: 0.506 - ETA: 1s - loss: 0.4217 - auc: 0.90 - ETA: 1s - loss: 0.4161 - auc: 0.9 - ETA: 1s - loss: 0.4065 - auc: 0.906 - ETA: 1s - loss: 0.4033 - auc: 0.90 - ETA: 1s - loss: 0.3956 \n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 6s 104ms/step - loss: 0.0904 - auc: 0.9938 - val_loss: 0.0526 - val_auc: 0.9973: - ETA: 1s - loss: 0.1063 - auc: 0. - ETA: 1s - loss: 0.1039 -  - ETA: 0s - loss: 0.0928 - auc: 0.9\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 6s 107ms/step - loss: 0.0400 - auc: 0.9986 - val_loss: 0.0424 - val_auc: 0.9979\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 6s 115ms/step - loss: 0.0232 - auc: 0.9993 - val_loss: 0.0463 - val_auc: 0.9983.0234 - auc: 0.99\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 6s 103ms/step - loss: 0.0159 - auc: 0.9996 - val_loss: 0.0479 - val_auc: 0.9979s - loss: 0.0163 - auc: 0.9\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 6s 102ms/step - loss: 0.0111 - auc: 0.9996 - val_loss: 0.0539 - val_auc: 0.9960 3s - loss: 0.0101 - auc - ETA: 2s - loss: 0.0101 - auc: 0.999 - ETA: 2s - loss: 0.0098 - auc: 0.999 - ETA: 2s - loss: 0.0095 - auc: 1 - ETA: 2s - loss: 0. - ETA: 0s - loss: 0.0119 - auc: \n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 6s 106ms/step - loss: 0.0085 - auc: 0.9997 - val_loss: 0.0528 - val_auc: 0.9958 ETA: 0s - loss: 0.0087 - au\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 6s 102ms/step - loss: 0.0074 - auc: 0.9997 - val_loss: 0.0615 - val_auc: 0.9959ETA: 0s - loss: 0.0077 - auc\n",
      "model5 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 23s 298ms/step - loss: 0.3439 - auc: 0.9260 - val_loss: 0.1591 - val_auc: 0.9928\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 15s 264ms/step - loss: 0.0833 - auc: 0.9930 - val_loss: 0.0536 - val_auc: 0.9975\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 15s 265ms/step - loss: 0.0406 - auc: 0.9979 - val_loss: 0.0504 - val_auc: 0.9977\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 15s 266ms/step - loss: 0.0240 - auc: 0.9991 - val_loss: 0.0633 - val_auc: 0.9955\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 14s 263ms/step - loss: 0.0165 - auc: 0.9998 - val_loss: 0.0628 - val_auc: 0.9968\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 16s 287ms/step - loss: 0.0114 - auc: 0.9996 - val_loss: 0.0706 - val_auc: 0.9936\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 16s 297ms/step - loss: 0.0092 - auc: 0.9994 - val_loss: 0.0749 - val_auc: 0.9933\n",
      "model6 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 41s 614ms/step - loss: 0.2587 - auc: 0.9598 - val_loss: 0.0896 - val_auc: 0.9962\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 33s 597ms/step - loss: 0.0536 - auc: 0.9968 - val_loss: 0.0496 - val_auc: 0.9963\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 35s 632ms/step - loss: 0.0300 - auc: 0.9989 - val_loss: 0.0637 - val_auc: 0.9963\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 33s 600ms/step - loss: 0.0176 - auc: 0.9995 - val_loss: 0.0940 - val_auc: 0.9921\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 33s 598ms/step - loss: 0.0146 - auc: 0.9996 - val_loss: 0.0861 - val_auc: 0.9935\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 33s 603ms/step - loss: 0.0112 - auc: 0.9996 - val_loss: 0.0953 - val_auc: 0.9936\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 34s 612ms/step - loss: 0.0092 - auc: 0.9994 - val_loss: 0.1019 - val_auc: 0.9928\n",
      "model7 \n",
      "\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 17s 177ms/step - loss: 0.3558 - auc: 0.9257 - val_loss: 0.1792 - val_auc: 0.9913\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 8s 146ms/step - loss: 0.0976 - auc: 0.9916 - val_loss: 0.0598 - val_auc: 0.9966\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 8s 150ms/step - loss: 0.0459 - auc: 0.9977 - val_loss: 0.0536 - val_auc: 0.9973\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 8s 146ms/step - loss: 0.0279 - auc: 0.9989 - val_loss: 0.0612 - val_auc: 0.9962\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 8s 150ms/step - loss: 0.0184 - auc: 0.9997 - val_loss: 0.0606 - val_auc: 0.9966\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 8s 145ms/step - loss: 0.0130 - auc: 0.9996 - val_loss: 0.0689 - val_auc: 0.9945\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 8s 146ms/step - loss: 0.0106 - auc: 0.9996 - val_loss: 0.0784 - val_auc: 0.9932\n",
      "model8 \n",
      "\n",
      "         units_rnn  precision\n",
      "0         [64, 64]   0.920000\n",
      "1            [128]   0.958333\n",
      "2       [128, 128]   0.926667\n",
      "3             [32]   0.958621\n",
      "4         [32, 32]   0.938776\n",
      "5     [64, 64, 64]   0.931507\n",
      "6  [128, 128, 128]   0.926667\n",
      "7     [32, 32, 32]   0.912752\n"
     ]
    }
   ],
   "source": [
    "parameters = { \"units_rnn\" : [[64,64],[128],[128,128],[32],[32,32], [64,64,64],[128,128,128],[32,32,32]]}    \n",
    "\n",
    "results = pd.DataFrame({\"units_rnn\" : [], \"precision\" : []})\n",
    "\n",
    "lenght = len(parameters[\"units_rnn\"])\n",
    "\n",
    "for i in range(lenght):\n",
    "    param = parameters.get(\"units_rnn\")[i]\n",
    "    model = model_rnn(sequences_matrix,Y_train_encoded,param, verbose = False)\n",
    "    y_pred = model.predict(test_sequences_matrix).argmax(axis = 1)\n",
    "    print('model' + str(i+1), \"\\n\")\n",
    "    \n",
    "    precision = precision_score(Y_test, y_pred,pos_label=1)\n",
    "    results = results.append({\"units_rnn\": param,\"precision\" : precision}, ignore_index = True)\n",
    "    \n",
    "print(results)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333987ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd90d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "eb2f7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moze jakis hyperopt na kerasie?, albo zwykla petla i daj slowniki w srodku w liscie, aby po nich chodzil jako parametry, moze tak sie da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poszukaj sposobu na unbalanced in textowych danych, wez wykorzystaj transfer learning i kilka roznych modeli nie tylko DL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
