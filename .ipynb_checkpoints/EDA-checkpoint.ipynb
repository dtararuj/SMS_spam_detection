{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "2db5fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675a6d4",
   "metadata": {},
   "source": [
    "1. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ab71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19602fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f073d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e24e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390da646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: v1, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v1'].value_counts()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505bf488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b8a7e",
   "metadata": {},
   "source": [
    "How we can see our data set is unbalanced, 86.5% to 13.5% percent, in favor of not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8148bac",
   "metadata": {},
   "source": [
    "2. Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "308cf964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let transform our labels to binary values\n",
    "Y = LabelEncoder().fit_transform(data['v1']).reshape(-1,1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "260c185e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['v2']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "1ec884c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8916"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check how many words we have in our dataset\n",
    "\n",
    "# tokenize the document\n",
    "words = []\n",
    "\n",
    "for i in range(0, X.shape[0]):\n",
    "    result = text_to_word_sequence(X[i])\n",
    "    words.extend(result)\n",
    "    \n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fb31e5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i      2351\n",
       "to     2242\n",
       "you    2150\n",
       "a      1433\n",
       "the    1328\n",
       "u      1172\n",
       "and     979\n",
       "in      898\n",
       "is      889\n",
       "me      802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let see top 10 words, whether there are some useless words or single characters\n",
    "pd.DataFrame(words).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd52300",
   "metadata": {},
   "source": [
    "How we can see, many of our popular words are stopwords, let remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "fb831d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Our stopwords which we'll remove are presented below\n",
    "#nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "37edfc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     v2  \\\n",
      "0     Go until jurong point, crazy.. Available only ...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3     U dun say so early hor... U c already then say...   \n",
      "4     Nah I don't think he goes to usf, he lives aro...   \n",
      "...                                                 ...   \n",
      "5567  This is the 2nd time we have tried 2 contact u...   \n",
      "5568              Will �_ b going to esplanade fr home?   \n",
      "5569  Pity, * was in mood for that. So...any other s...   \n",
      "5570  The guy did some bitching but I acted like i'd...   \n",
      "5571                         Rofl. Its true to its name   \n",
      "\n",
      "                                                  clean  \n",
      "0     Go jurong point, crazy.. Available bugis n gre...  \n",
      "1                         Ok lar... Joking wif u oni...  \n",
      "2     Free entry 2 wkly comp win FA Cup final tkts 2...  \n",
      "3             U dun say early hor... U c already say...  \n",
      "4               Nah think goes usf, lives around though  \n",
      "...                                                 ...  \n",
      "5567  2nd time tried 2 contact u. U �750 Pound prize...  \n",
      "5568                      �_ b going esplanade fr home?  \n",
      "5569           Pity, * mood that. So...any suggestions?  \n",
      "5570  guy bitching acted like i'd interested buying ...  \n",
      "5571                                    Rofl. true name  \n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "df['clean'] = df['v2'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (stop_words)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7f0cf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let remove stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # tokenize our text\n",
    "# X_clean = []\n",
    "\n",
    "# for i in range(X.shape[0]):\n",
    "#     #tokenize each sentences\n",
    "#     word_tokens = text_to_word_sequence(X[i])\n",
    "#     filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] # filter stop words from orignal sentences\n",
    "#     X_clean.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "2bc06327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u       1172\n",
       "call     591\n",
       "2        515\n",
       "i'm      394\n",
       "get      391\n",
       "ur       385\n",
       "gt       318\n",
       "lt       316\n",
       "4        316\n",
       "ok       287\n",
       "free     284\n",
       "go       280\n",
       "know     261\n",
       "now      255\n",
       "good     245\n",
       "like     245\n",
       "got      239\n",
       "you      233\n",
       "come     229\n",
       "time     220\n",
       "dtype: int64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check again if there are still any useless words not included in stopwords\n",
    "\n",
    "# tokenize the document\n",
    "words = []\n",
    "df = df['clean']\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    result = text_to_word_sequence(df[i])\n",
    "    words.extend(result)\n",
    "    \n",
    "len(set(words))\n",
    "\n",
    "    \n",
    "pd.DataFrame(words).value_counts().head(20)\n",
    "#there are still some stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "175ed4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  clean  \\\n",
      "0     Go jurong point, crazy.. Available bugis n gre...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry 2 wkly comp win FA Cup final tkts 2...   \n",
      "3             U dun say early hor... U c already say...   \n",
      "4               Nah think goes usf, lives around though   \n",
      "...                                                 ...   \n",
      "5567  2nd time tried 2 contact u. U �750 Pound prize...   \n",
      "5568                      �_ b going esplanade fr home?   \n",
      "5569           Pity, * mood that. So...any suggestions?   \n",
      "5570  guy bitching acted like i'd interested buying ...   \n",
      "5571                                    Rofl. true name   \n",
      "\n",
      "                                                clean_1  \n",
      "0     Go jurong point, crazy.. Available bugis great...  \n",
      "1                           Ok lar... Joking wif oni...  \n",
      "2     Free entry wkly comp win FA Cup final tkts 21s...  \n",
      "3                 dun say early hor... c already say...  \n",
      "4               Nah think goes usf, lives around though  \n",
      "...                                                 ...  \n",
      "5567  2nd time tried contact u. �750 Pound prize. cl...  \n",
      "5568                      �_ b going esplanade fr home?  \n",
      "5569           Pity, * mood that. So...any suggestions?  \n",
      "5570  guy bitching acted like i'd interested buying ...  \n",
      "5571                                    Rofl. true name  \n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# let remove another stopwords, not defined before\n",
    "\n",
    "more_stopwords = set(['u','4','2',\"i'm\",\"i'll\",'r','ur','n'])\n",
    "updated_step_words = stop_words | more_stopwords\n",
    "\n",
    "df1 = pd.DataFrame(df)\n",
    "\n",
    "df1['clean_1'] = df1['clean'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (updated_step_words)]))\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "a4013f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let split our dataset on train and test\n",
    "\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(df1['clean_1'],Y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab34077",
   "metadata": {},
   "source": [
    "3. Text tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "a20afc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let check the longest sms in our dataset\n",
    "maxims = []\n",
    "for i,j in enumerate(X_clean):\n",
    "    maxims.append(len(X_clean[i]))\n",
    "    \n",
    "np.max(maxims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "02a88a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let create tokenizer\n",
    "max_words = 800\n",
    "max_len = 76\n",
    "\n",
    "tok = Tokenizer(max_words,lower= True)\n",
    "tok.fit_on_texts(X_train) #token trainging on X_train\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train) #conversion from text to vectors\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len) #matrix with padding has been created\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "1d952b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let check records where after tokenizing we have only zeros\n",
    "test_0 = pd.DataFrame({\"X\" : test_sequences_matrix.argmax(axis=1)})\n",
    "test_0_filtred = test_0[test_0[\"X\"]<1]\n",
    "\n",
    "test_to_remove = test_0_filtred.index\n",
    "\n",
    "train_0 = pd.DataFrame({\"X\" : sequences_matrix.argmax(axis=1)})\n",
    "train_0_filtred = train_0[train_0[\"X\"]<1]\n",
    "\n",
    "train_to_remove = train_0_filtred.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "6225139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let remove those zeros tokens from our dataset \n",
    "\n",
    "#X_train \n",
    "sequences_matrix = np.delete(sequences_matrix, train_to_remove,axis = 0 )\n",
    "\n",
    "#X_test\n",
    "test_sequences_matrix =  np.delete(test_sequences_matrix, test_to_remove,axis = 0 )\n",
    "\n",
    "#Y_train\n",
    "Y_train = np.delete(Y_train, train_to_remove, axis = 0)\n",
    "\n",
    "#Y_test\n",
    "Y_test = np.delete(Y_test, test_to_remove, axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a87fdd",
   "metadata": {},
   "source": [
    "4. First models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "c3bcf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make reproduced results.\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1da7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forrest, knn, svm, rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ed23f",
   "metadata": {},
   "source": [
    "a) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "6d370648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       948\n",
      "           1       0.75      0.37      0.50       139\n",
      "\n",
      "    accuracy                           0.90      1087\n",
      "   macro avg       0.83      0.68      0.72      1087\n",
      "weighted avg       0.89      0.90      0.89      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model1_svm = make_pipeline(MinMaxScaler(),svm.SVC())\n",
    "model1_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_svm = model1_svm.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd22525",
   "metadata": {},
   "source": [
    "As we can see, model predict not-spam class very well, but reach only 50% f1_score for spam class. \n",
    "We are going to improve this model a little bit, by changing default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "d6363656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       948\n",
      "           1       0.77      0.36      0.49       139\n",
      "\n",
      "    accuracy                           0.90      1087\n",
      "   macro avg       0.84      0.67      0.72      1087\n",
      "weighted avg       0.89      0.90      0.89      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another simple model\n",
    "\n",
    "model2_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'poly'))\n",
    "model2_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_svm = model2_svm.predict(test_sequences_matrix)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "9b1c6d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       948\n",
      "           1       0.62      0.32      0.42       139\n",
      "\n",
      "    accuracy                           0.89      1087\n",
      "   macro avg       0.76      0.65      0.68      1087\n",
      "weighted avg       0.87      0.89      0.87      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another simple model\n",
    "\n",
    "model3_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'linear'))\n",
    "model3_svm.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred3_svm = model3_svm.predict(test_sequences_matrix)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred3_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0770237",
   "metadata": {},
   "source": [
    "Three basic models with default parameters, only with different kernel don't reach sufficient results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2f80d",
   "metadata": {},
   "source": [
    "b) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "9a70068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94       948\n",
      "           1       0.60      0.38      0.47       139\n",
      "\n",
      "    accuracy                           0.89      1087\n",
      "   macro avg       0.76      0.67      0.70      1087\n",
      "weighted avg       0.87      0.89      0.88      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model1_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n",
    "model1_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred1_knn = model1_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred1_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "c3f0aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       948\n",
      "           1       0.74      0.53      0.62       139\n",
      "\n",
      "    accuracy                           0.92      1087\n",
      "   macro avg       0.84      0.75      0.79      1087\n",
      "weighted avg       0.91      0.92      0.91      1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple model as benchmark\n",
    "\n",
    "model2_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier(weights = 'distance'))\n",
    "model2_knn.fit(sequences_matrix, Y_train.flatten())\n",
    "\n",
    "y_pred2_knn = model2_knn.predict(test_sequences_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(Y_test.flatten(),y_pred2_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a3dac",
   "metadata": {},
   "source": [
    "How we can see first, basic KNN model didn't improve our previous results, but with only one change in weight parameters, we beat out previous best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a2098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2799fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f100b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "fcb5d8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 620, 665, 218],\n",
       "       [  0,   0,   0, ..., 666,  33, 138],\n",
       "       [  0,   0,   0, ...,   0, 139,  68],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0, 400, 637],\n",
       "       [  0,   0,   0, ..., 171,  56, 112],\n",
       "       [  0,   0,   0, ...,  25,  38,  45]])"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad631a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c25667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf9b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb195f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poszukaj sposobu na unbalanced in textowych danych, wez wykorzystaj transfer learning i kilka roznych modeli nie tylko DL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
